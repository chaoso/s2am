{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total Dataset of real_data is :  99\n",
      "('test01_Su.png',)\n",
      "('test02_Su.png',)\n",
      "('test03_Su.png',)\n",
      "('test04_Su.png',)\n",
      "('test05_Su.png',)\n",
      "('test06_Su.png',)\n",
      "('test07_Su.png',)\n",
      "('test08_Su.png',)\n",
      "('test09_Su.png',)\n",
      "('test10_Su.png',)\n",
      "('test11_Su.png',)\n",
      "('test12_Su.png',)\n",
      "('test13_Su.png',)\n",
      "('test14_Su.png',)\n",
      "('test15_Su.png',)\n",
      "('test16_Su.png',)\n",
      "('test17_Su.png',)\n",
      "('test18_Su.png',)\n",
      "('test19_Su.png',)\n",
      "('test20_Su.png',)\n",
      "('test21_Su.png',)\n",
      "('test22_Su.png',)\n",
      "('test23_Su.png',)\n",
      "('test24_Su.png',)\n",
      "('test25_Su.png',)\n",
      "('test26_Su.png',)\n",
      "('test27_Su.png',)\n",
      "('test28_Su.png',)\n",
      "('test29_Su.png',)\n",
      "('test30_Su.png',)\n",
      "('test31_Su.png',)\n",
      "('test32_Su.png',)\n",
      "('test33_Su.png',)\n",
      "('test34_Su.png',)\n",
      "('test35_Su.png',)\n",
      "('test36_Su.png',)\n",
      "('test37_Su.png',)\n",
      "('test38_Su.png',)\n",
      "('test39_Su.png',)\n",
      "('test40_Su.png',)\n",
      "('test41_Su.png',)\n",
      "('test42_Su.png',)\n",
      "('test43_Su.png',)\n",
      "('test44_Su.png',)\n",
      "('test45_Su.png',)\n",
      "('test46_Su.png',)\n",
      "('test47_Su.png',)\n",
      "('test48_Su.png',)\n",
      "('test_01.png',)\n",
      "('test_02.png',)\n",
      "('test_03.png',)\n",
      "('test_04.png',)\n",
      "('test_05.png',)\n",
      "('test_06.png',)\n",
      "('test_07.png',)\n",
      "('test_08.png',)\n",
      "('test_09.png',)\n",
      "('test_10.png',)\n",
      "('test_11.png',)\n",
      "('test_12.png',)\n",
      "('test_13.png',)\n",
      "('test_14.png',)\n",
      "('test_15.png',)\n",
      "('test_16.png',)\n",
      "('test_17.png',)\n",
      "('test_18.png',)\n",
      "('test_19.png',)\n",
      "('test_20.png',)\n",
      "('test_21.png',)\n",
      "('test_22.png',)\n",
      "('test_23.png',)\n",
      "('test_24.png',)\n",
      "('test_25.png',)\n",
      "('test_26.png',)\n",
      "('test_27.png',)\n",
      "('test_28.png',)\n",
      "('test_29.png',)\n",
      "('test_30.png',)\n",
      "('test_31.png',)\n",
      "('test_32.png',)\n",
      "('test_33.png',)\n",
      "('test_34.png',)\n",
      "('test_35.png',)\n",
      "('test_36.png',)\n",
      "('test_37.png',)\n",
      "('test_38.png',)\n",
      "('test_39.png',)\n",
      "('test_40.png',)\n",
      "('test_41.png',)\n",
      "('test_42.png',)\n",
      "('test_43.png',)\n",
      "('test_44.png',)\n",
      "('test_45.png',)\n",
      "('test_46.png',)\n",
      "('test_47.png',)\n",
      "('test_48.png',)\n",
      "('test_49.png',)\n",
      "('test_50.png',)\n",
      "('test_51.png',)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "\n",
    "\n",
    "import os \n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageChops\n",
    "import sys, math\n",
    "sys.path.append('..')\n",
    "\n",
    "import scripts.utils\n",
    "from scripts.utils.logger import Logger, savefig\n",
    "from scripts.utils.evaluation import accuracy, AverageMeter, final_preds\n",
    "from scripts.utils.misc import save_checkpoint, save_pred, adjust_learning_rate\n",
    "from scripts.utils.osutils import mkdir_p, isfile, isdir, join\n",
    "from scripts.utils.imutils import batch_with_heatmap,normalize_batch,im_to_numpy\n",
    "from scripts.utils.transforms import fliplr, flip_back\n",
    "import scripts.models as models\n",
    "import scripts.datasets as datasets\n",
    "import random\n",
    "from matplotlib import cm\n",
    "from scripts.utils.pytorch_modelsize import SizeEstimator\n",
    " \n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def get_jet():\n",
    "    colormap_int = np.zeros((256, 3), np.uint8)\n",
    " \n",
    "    for i in range(0, 256, 1):\n",
    "        colormap_int[i, 0] = np.int_(np.round(cm.jet(i)[0] * 255.0))\n",
    "        colormap_int[i, 1] = np.int_(np.round(cm.jet(i)[1] * 255.0))\n",
    "        colormap_int[i, 2] = np.int_(np.round(cm.jet(i)[2] * 255.0))\n",
    "\n",
    "    return colormap_int\n",
    "\n",
    "jet_map = get_jet()\n",
    "\n",
    "def clamp(num, min_value, max_value):\n",
    "    return max(min(num, max_value), min_value)\n",
    "\n",
    "def gray2color(gray_array, color_map):\n",
    "    \n",
    "    rows, cols = gray_array.shape\n",
    "    color_array = np.zeros((rows, cols, 3), np.uint8)\n",
    " \n",
    "    for i in range(0, rows):\n",
    "        for j in range(0, cols):\n",
    "#             log(256,2) = 8 , log(1,2) = 0 * 8\n",
    "            color_array[i, j] = color_map[clamp(int(abs(gray_array[i, j])*10),0,255)]\n",
    "    \n",
    "    return color_array\n",
    "\n",
    "\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "from skimage.measure import compare_psnr as psnr\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        d = dict(*args, **kwargs)\n",
    "        self.__dict__ = d\n",
    "\n",
    "dataset_name= 'real_data'\n",
    "dataroot = '/home/oishii/Datasets/backup/'\n",
    "input_size = 512\n",
    "\n",
    "data_config_normal  = objectview({'input_size':input_size,\n",
    "                           'limited_dataset':0,\n",
    "                           'resize_and_crop':'resize',\n",
    "                           'data_augumentation':False,\n",
    "                           'norm_type':'none',\n",
    "                           'base_dir':dataroot,\n",
    "                           'data':dataset_name})\n",
    "\n",
    "data_config_gan  = objectview({'input_size':input_size,\n",
    "                           'limited_dataset':0,\n",
    "                           'resize_and_crop':'resize',\n",
    "                           'norm_type':'gan',\n",
    "                           'base_dir':dataroot,\n",
    "                           'data':dataset_name})\n",
    "\n",
    "\n",
    "methods = [\n",
    "#            ('/home/oishii/Documents/imageharmonization/rasc-v2/limited/cocov4/1e3_bs16_256_basic_2017_naiveuno',data_config_normal),\n",
    "#            ('/home/oishii/Documents/imageharmonization/rasc-v2/limited/cocov2/1e3_bs16_256_gradient9_basic_2017_naiveuno',data_config_normal),    \n",
    "#             ('/home/oishii/Documents/imageharmonization/rasc-v2/limited/cocov4/1e3_bs16_256_multimaskedpixel_2017_naivemmucross',data_config_normal),\n",
    "#             ('/home/oishii/Documents/imageharmonization/rasc-v2/limited/cocov4gan/1e3_bs16_256_mmaskedgan_2017_naivemmucross',data_config_gan),\n",
    "#     ('/home/oishii/Documents/imageharmonization/rasc-v2/limited/cocov4gan/1e3_bs16_256_baseline_mmaskedgan_2017_naivemmucross',data_config_gan),\n",
    "        ('/home/oishii/Documents/imageharmonization/rasc-v2/psnr/cocov4/1e3_bs4_512_basic_final_rascv2512',data_config_normal),\n",
    "]\n",
    "\n",
    "#    [905]\n",
    "# \n",
    "\n",
    "# sample =random.sample(range(80), 1); # 201 712 668 504 [843] #\n",
    "# sample = [27]\n",
    "\n",
    "sample=[]\n",
    "\n",
    "#             675,(val2014,271), (val2014,949)\n",
    "# run with gan-based method            \n",
    "with torch.no_grad():\n",
    "    for resume,data_config in methods:\n",
    "        os.makedirs(os.path.join(resume,dataset_name),exist_ok=True)\n",
    "        dataloader = datasets.COCO\n",
    "        val_loader = torch.utils.data.DataLoader(dataloader('',data_config,sample=sample),\n",
    "                                             batch_size=1, shuffle=False,\n",
    "                                             num_workers=2, pin_memory=False)\n",
    "        epoches = '/model_best.pth.tar'\n",
    "        checkpoint_dict = torch.load(resume+epoches)\n",
    "        checkpoint = checkpoint_dict['state_dict']\n",
    "            \n",
    "        for i, (inputs, target, name_of_img) in enumerate(val_loader):            \n",
    "            \n",
    "            name_of_model = resume.split('_')[-1]\n",
    "           \n",
    "            model = models.__dict__[name_of_model]()\n",
    "            model.load_state_dict(checkpoint)\n",
    "            model.eval()\n",
    "            \n",
    "            inputs = inputs\n",
    "            target = target[0]\n",
    "            \n",
    "            if 'rascv2' in resume:\n",
    "                output = model(inputs)\n",
    "                mask2s = inputs[0,3:4]\n",
    "            elif 'gagan' in resume:\n",
    "                output,mask2s = model(inputs)\n",
    "            else:\n",
    "                output,mask8s,mask4s,mask2s = model(inputs)\n",
    "                mask2s = nn.functional.interpolate(mask2s,scale_factor=2)\n",
    "               \n",
    "            # if gan based method\n",
    "            if 'gan' in data_config.norm_type:\n",
    "                outputnp = (im_to_numpy(output[0]) + 1) / 2.0 * 255.0\n",
    "                inputsnp = (im_to_numpy(inputs[0,0:3]) + 1) / 2.0 * 255.0\n",
    "                targetnp = (im_to_numpy(target) + 1) / 2.0 * 255.0\n",
    "                outputnp = outputnp.clip(0,255).astype(np.uint8)\n",
    "                inputsnp = inputsnp.clip(0,255).astype(np.uint8)\n",
    "                targetnp = targetnp.clip(0,255).astype(np.uint8)\n",
    "            else:\n",
    "                inputsnp = (im_to_numpy(inputs[0,0:3])*255).clip(0,255).astype(np.uint8)\n",
    "                outputnp = (im_to_numpy(output[0])*255).clip(0,255).astype(np.uint8)\n",
    "                targetnp = (im_to_numpy(target)*255).clip(0,255).astype(np.uint8)\n",
    "                \n",
    "                      \n",
    "            im_output = Image.fromarray(outputnp)\n",
    "#             im_input = Image.fromarray(inputsnp)\n",
    "#             im_target = Image.fromarray(targetnp)\n",
    "#             im_mask = Image.fromarray(masknp)\n",
    "#             im_attention = Image.fromarray(attentionmap)\n",
    "            \n",
    "#             jetnp = gray2color(np.array(ImageChops.difference(im_target,im_output).convert('L')),jet_map)\n",
    "#             im_jet = Image.fromarray(jetnp)\n",
    "            \n",
    "#             tmp_ssim = ssim(targetnp,outputnp,multichannel=True)\n",
    "#             tmp_psnr = psnr(targetnp,outputnp)\n",
    "#             tmp_mse = np.mean( (outputnp - targetnp) ** 2 )\n",
    "  \n",
    "            im_output.save(name_of_img[0])\n",
    "#             im_jet.save(resume.split('/')[-1].replace('1e3_bs4_512','').replace('_final_','')+'jet_'+str(sample)+'.png')\n",
    "#             im_attention.save(resume.split('/')[-1].replace('1e3_bs4_512','').replace('_final_','')+'att_'+str(sample)+'.png')\n",
    "#             im_input.save('input_'+str(sample)+'.png')\n",
    "#             im_target.save('target_'+str(sample)+'.png')\n",
    "#             im_mask.save('mask_'+str(sample)+'.png')\n",
    "            \n",
    "#             print(resume.split('/')[-1],tmp_ssim,tmp_psnr,tmp_mse,sample)\n",
    "#             plt.figure(figsize=(30,6))\n",
    "#             plt.xlabel(resume.split('/')[-1])\n",
    "#             plt.imshow(np.concatenate((inputsnp,outputnp,targetnp,jetnp,attentionmap),axis=1))\n",
    "#             plt.draw()\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
